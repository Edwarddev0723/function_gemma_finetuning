# â˜• FunctionGemma Coffee Robot MCP Fine-tuning

åŸºæ–¼ Google FunctionGemma 270M æ¨¡å‹ï¼Œé‡å°å’–å•¡æ©Ÿå™¨äººå¤–é€ä»»å‹™é€²è¡Œ Function Calling å¾®èª¿çš„å®Œæ•´å°ˆæ¡ˆã€‚

![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)
![Transformers](https://img.shields.io/badge/Transformers-4.57+-yellow.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)

## ğŸ“‹ å°ˆæ¡ˆæ¦‚è¿°

æ­¤å°ˆæ¡ˆå±•ç¤ºå¦‚ä½•ï¼š
1. ä½¿ç”¨æœ¬åœ° LLM (Ollama) è‡ªå‹•ç”Ÿæˆé«˜å“è³ªçš„ Function Calling è¨“ç·´è³‡æ–™
2. å¾®èª¿ FunctionGemma æ¨¡å‹ä»¥é©æ‡‰ç‰¹å®šé ˜åŸŸä»»å‹™
3. åœ¨ Apple Silicon (M4 Pro) ä¸Šé€²è¡Œé«˜æ•ˆè¨“ç·´

### æ‡‰ç”¨å ´æ™¯
å°‡è‡ªç„¶èªè¨€æŒ‡ä»¤è½‰æ›ç‚ºçµæ§‹åŒ–çš„å’–å•¡å¤–é€ä»»å‹™ï¼š

```
ä½¿ç”¨è€…ï¼šã€Œå¹«æˆ‘é€ä¸€æ¯ç†±ç¾å¼åˆ°äº”æ¨“ã€

â†“ FunctionGemma å¾®èª¿æ¨¡å‹ â†“

Function Call: create_coffee_robot_mission({
    "baseDrink": "americano",
    "floor": 5,
    "temperature": "hot",
    "quantity": 1
})
```

## ğŸ—‚ï¸ å°ˆæ¡ˆçµæ§‹

```
function_gemma_finetuning/
â”œâ”€â”€ README.md                                    # å°ˆæ¡ˆèªªæ˜æ–‡ä»¶
â”œâ”€â”€ .gitignore                                   # Git å¿½ç•¥è¦å‰‡
â”œâ”€â”€ dataset_prepare.py                           # è³‡æ–™é›†ç”Ÿæˆè…³æœ¬
â”œâ”€â”€ ollama_mcp_dataset.jsonl                     # ç”Ÿæˆçš„è¨“ç·´è³‡æ–™é›†
â”œâ”€â”€ Finetune_FunctionGemma_Coffee_Robot_MCP.ipynb # å¾®èª¿è¨“ç·´ Notebook
â”œâ”€â”€ eval_base_model.json                         # åŸºç¤æ¨¡å‹è©•ä¼°çµæœ
â”œâ”€â”€ eval_trained_model.json                      # å¾®èª¿æ¨¡å‹è©•ä¼°çµæœ
â””â”€â”€ coffee-robot-functiongemma/                  # å¾®èª¿å¾Œçš„æ¨¡å‹è¼¸å‡º
    â”œâ”€â”€ config.json
    â”œâ”€â”€ model.safetensors
    â”œâ”€â”€ tokenizer.json
    â””â”€â”€ ...
```

## ğŸš€ å¿«é€Ÿé–‹å§‹

### ç’°å¢ƒéœ€æ±‚

- Python 3.11+
- PyTorch 2.0+
- 48GB+ RAM (å»ºè­°ï¼Œç”¨æ–¼ Apple Silicon)
- [Ollama](https://ollama.ai/) (ç”¨æ–¼ç”Ÿæˆè³‡æ–™é›†)

### å®‰è£å¥—ä»¶

```bash
pip install torch
pip install transformers==4.57.1 trl==0.25.1 datasets==4.4.1
pip install matplotlib pandas tqdm requests
```

### æ­¥é©Ÿ 1ï¼šç”Ÿæˆè¨“ç·´è³‡æ–™é›†

ç¢ºä¿ Ollama æœå‹™å·²å•Ÿå‹•ï¼š

```bash
ollama serve
```

åŸ·è¡Œè³‡æ–™é›†ç”Ÿæˆè…³æœ¬ï¼š

```bash
python dataset_prepare.py
```

é€™æœƒä½¿ç”¨æœ¬åœ° LLM ç”Ÿæˆ 1000 ç­†å’–å•¡å¤–é€ä»»å‹™çš„ Function Calling ç¯„ä¾‹ã€‚

### æ­¥é©Ÿ 2ï¼šå¾®èª¿æ¨¡å‹

é–‹å•Ÿ Jupyter Notebook ä¸¦åŸ·è¡Œï¼š

```bash
jupyter notebook Finetune_FunctionGemma_Coffee_Robot_MCP.ipynb
```

æˆ–åœ¨ VS Code ä¸­ç›´æ¥é–‹å•Ÿ `.ipynb` æª”æ¡ˆã€‚

## ğŸ› ï¸ æ”¯æ´çš„ Function

### `create_coffee_robot_mission`

| åƒæ•¸ | é¡å‹ | èªªæ˜ | å¯é¸å€¼ |
|------|------|------|--------|
| `baseDrink` | string | é£²å“ç¨®é¡ | `americano`, `latte`, `oat_latte`, `milk` |
| `floor` | integer | é…é€æ¨“å±¤ | 1-11 |
| `temperature` | string | é£²å“æº«åº¦ | `hot`, `iced` |
| `addons` | array | åŠ è³¼é¸é … | `extra_espresso`, `paper_cup` |
| `quantity` | integer | æ•¸é‡ | 1-10 |

## ğŸ“Š è³‡æ–™é›†æ ¼å¼

è¨“ç·´è³‡æ–™æ¡ç”¨ JSONL æ ¼å¼ï¼Œæ¯ç­†è³‡æ–™åŒ…å«ï¼š

```json
{
  "messages": [
    {"role": "user", "content": "å¹«æˆ‘é€ä¸€æ¯ç†±ç¾å¼åˆ°äº”æ¨“"},
    {"role": "assistant", "tool_calls": [...]}
  ],
  "tools": [...],
  "metadata": "train"
}
```

## âš™ï¸ è¨“ç·´é…ç½®

é‡å° Apple Silicon M4 Pro 48GB å„ªåŒ–çš„è¨“ç·´åƒæ•¸ï¼š

| åƒæ•¸ | å€¼ | èªªæ˜ |
|------|-----|------|
| `per_device_train_batch_size` | 1 | æ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨ |
| `gradient_accumulation_steps` | 16 | æœ‰æ•ˆæ‰¹æ¬¡å¤§å° = 16 |
| `learning_rate` | 1e-5 | å­¸ç¿’ç‡ |
| `num_train_epochs` | 3 | è¨“ç·´è¼ªæ•¸ |
| `gradient_checkpointing` | True | ç¯€çœè¨˜æ†¶é«” |
| `optim` | adamw_torch | MPS ç›¸å®¹å„ªåŒ–å™¨ |

## ğŸ’» ä½¿ç”¨å¾®èª¿å¾Œçš„æ¨¡å‹

```python
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# è¼‰å…¥æ¨¡å‹
model_path = "./coffee-robot-functiongemma"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path, device_map="auto")

# å»ºç«‹ pipeline
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)

# å®šç¾©å·¥å…·
tools = [{
    "name": "create_coffee_robot_mission",
    "description": "å»ºç«‹å’–å•¡æ©Ÿå™¨äººå¤–é€ä»»å‹™",
    "parameters": {
        "type": "object",
        "properties": {
            "baseDrink": {"type": "string", "enum": ["americano", "latte", "oat_latte", "milk"]},
            "floor": {"type": "integer", "minimum": 1, "maximum": 11},
            "temperature": {"type": "string", "enum": ["hot", "iced"]},
            "addons": {"type": "array", "items": {"type": "string"}},
            "quantity": {"type": "integer"}
        },
        "required": ["baseDrink", "floor"]
    }
}]

# æ¨è«–
messages = [{"role": "user", "content": "å¹«æˆ‘é€ä¸‰æ¯å†°æ‹¿éµåˆ°ä¸ƒæ¨“"}]
prompt = tokenizer.apply_chat_template(messages, tools=tools, tokenize=False, add_generation_prompt=True)
output = pipe(prompt, max_new_tokens=256)
print(output[0]['generated_text'][len(prompt):])
```

## ğŸ“ˆ æ¨¡å‹æ•ˆèƒ½

| æŒ‡æ¨™ | åŸºç¤æ¨¡å‹ | å¾®èª¿å¾Œæ¨¡å‹ |
|------|----------|------------|
| Function Calling æº–ç¢ºç‡ | ~10% | ~95%+ |

## ğŸ”— ç›¸é—œè³‡æº

- [FunctionGemma æ¨¡å‹](https://huggingface.co/google/functiongemma-270m-it)
- [TRL (Transformer Reinforcement Learning)](https://github.com/huggingface/trl)
- [Ollama](https://ollama.ai/)
- [Model Context Protocol (MCP)](https://modelcontextprotocol.io/)

## ğŸ“ æˆæ¬Š

æœ¬å°ˆæ¡ˆç¨‹å¼ç¢¼æ¡ç”¨ MIT æˆæ¬Šã€‚

å¾®èª¿æ¨¡å‹åŸºæ–¼ [Gemma License](https://ai.google.dev/gemma/terms)ï¼Œä½¿ç”¨å‰è«‹ç¢ºèªå·²æ¥å— Google Gemma çš„ä½¿ç”¨æ¢æ¬¾ã€‚

## ğŸ™ è‡´è¬

- [Google](https://ai.google.dev/) - æä¾› FunctionGemma åŸºç¤æ¨¡å‹
- [Hugging Face](https://huggingface.co/) - æä¾›æ¨¡å‹è¨—ç®¡èˆ‡è¨“ç·´å·¥å…·
- [Ollama](https://ollama.ai/) - æä¾›æœ¬åœ° LLM æœå‹™

---

Made with â˜• by Edward Huang
