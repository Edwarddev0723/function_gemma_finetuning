{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "194b6918",
   "metadata": {},
   "source": [
    "## 1. å®‰è£å¿…è¦å¥—ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0490feab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers==4.57.1 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (4.57.1)\n",
      "Collecting trl==0.25.1\n",
      "  Downloading trl-0.25.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting datasets==4.4.1\n",
      "  Using cached datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from transformers==4.57.1) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from transformers==4.57.1) (0.34.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from transformers==4.57.1) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from transformers==4.57.1) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from transformers==4.57.1) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from transformers==4.57.1) (2025.11.3)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from transformers==4.57.1) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from transformers==4.57.1) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from transformers==4.57.1) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from transformers==4.57.1) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=1.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from trl==0.25.1) (1.10.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from datasets==4.4.1) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from datasets==4.4.1) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from datasets==4.4.1) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from datasets==4.4.1) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from datasets==4.4.1) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from datasets==4.4.1) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets==4.4.1) (2025.10.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets==4.4.1) (3.11.10)\n",
      "Requirement already satisfied: anyio in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from httpx<1.0.0->datasets==4.4.1) (4.9.0)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from httpx<1.0.0->datasets==4.4.1) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from httpx<1.0.0->datasets==4.4.1) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from httpx<1.0.0->datasets==4.4.1) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0.0->datasets==4.4.1) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.1) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.1) (1.2.0)\n",
      "Requirement already satisfied: psutil in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from accelerate>=1.4.0->trl==0.25.1) (7.1.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from accelerate>=1.4.0->trl==0.25.1) (2.7.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets==4.4.1) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets==4.4.1) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets==4.4.1) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets==4.4.1) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets==4.4.1) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets==4.4.1) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets==4.4.1) (1.18.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from requests->transformers==4.57.1) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from requests->transformers==4.57.1) (2.5.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.25.1) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.25.1) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.25.1) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=1.4.0->trl==0.25.1) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from anyio->httpx<1.0.0->datasets==4.4.1) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl==0.25.1) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from pandas->datasets==4.4.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from pandas->datasets==4.4.1) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from pandas->datasets==4.4.1) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets==4.4.1) (1.17.0)\n",
      "Downloading trl-0.25.1-py3-none-any.whl (465 kB)\n",
      "Using cached datasets-4.4.1-py3-none-any.whl (511 kB)\n",
      "Installing collected packages: datasets, trl\n",
      "\u001b[2K  Attempting uninstall: datasets\n",
      "\u001b[2K    Found existing installation: datasets 4.2.0\n",
      "\u001b[2K    Uninstalling datasets-4.2.0:\n",
      "\u001b[2K      Successfully uninstalled datasets-4.2.0\n",
      "\u001b[2K  Attempting uninstall: trlâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0/2\u001b[0m [datasets]\n",
      "\u001b[2K    Found existing installation: trl 0.22.2â”\u001b[0m \u001b[32m0/2\u001b[0m [datasets]\n",
      "\u001b[2K    Uninstalling trl-0.22.2:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0/2\u001b[0m [datasets]\n",
      "\u001b[2K      Successfully uninstalled trl-0.22.2â”â”â”\u001b[0m \u001b[32m0/2\u001b[0m [datasets]\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2/2\u001b[0m [trl]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed datasets-4.4.1 trl-0.25.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (3.10.0)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (2.3.3)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from matplotlib) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install -U transformers==4.57.1 trl==0.25.1 datasets==4.4.1\n",
    "%pip install matplotlib pandas tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be68b4e",
   "metadata": {},
   "source": [
    "## 2. Hugging Face ç™»å…¥\n",
    "\n",
    "è«‹ç¢ºä¿ä½ å·²ç¶“ï¼š\n",
    "1. åœ¨ [FunctionGemma æ¨¡å‹é é¢](http://huggingface.co/google/functiongemma-270m-it) æ¥å—ä½¿ç”¨æ¢æ¬¾\n",
    "2. å–å¾— [Access Token](https://huggingface.co/settings/tokens) (éœ€è¦ 'Write' æ¬Šé™)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d558c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ç™»å…¥æˆåŠŸï¼\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# æ–¹æ³• 1: ä½¿ç”¨ç’°å¢ƒè®Šæ•¸\n",
    "hf_token = os.environ.get('HF_TOKEN')\n",
    "\n",
    "# æ–¹æ³• 2: å¦‚æœæ²’æœ‰ç’°å¢ƒè®Šæ•¸ï¼Œæ‰‹å‹•è¼¸å…¥\n",
    "if not hf_token:\n",
    "    hf_token = input(\"è«‹è¼¸å…¥ä½ çš„ Hugging Face Token: \")\n",
    "\n",
    "login(hf_token)\n",
    "print(\"âœ… ç™»å…¥æˆåŠŸï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70d216a",
   "metadata": {},
   "source": [
    "## 3. è¼‰å…¥ FunctionGemma æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97f195cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "242f35af938549d082d4af2bfadb7671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59a4b65c72a5441f898243f20b3ae09d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "633191205d3a4128946e49789a986919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "402388774e9c408fb5a60c6f665794d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/63.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfee0098fee04056b65c4accc7806b12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/706 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8beeb91e56d4e0abc934eee5f08271a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja:   0%|          | 0.00/13.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba742ec852574c41b6e0c9ff82b27f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "805939fc9b784cd8af2b8766a633db2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/536M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b6e70ba6ab47e59ba45e317f32fecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/176 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps:0\n",
      "DType:  torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "gemma_model = \"google/functiongemma-270m-it\"\n",
    "\n",
    "# è¼‰å…¥ tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(gemma_model)\n",
    "\n",
    "# è¼‰å…¥æ¨¡å‹\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    gemma_model,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"Device: {base_model.device}\")\n",
    "print(f\"DType:  {base_model.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c347d04",
   "metadata": {},
   "source": [
    "## 4. è¼‰å…¥ Coffee Robot MCP è³‡æ–™é›†\n",
    "\n",
    "è¼‰å…¥æˆ‘å€‘ç”¨ Ollama ç”Ÿæˆçš„ `ollama_mcp_dataset.jsonl` è³‡æ–™é›†ã€‚\n",
    "\n",
    "è³‡æ–™é›†åŒ…å«å’–å•¡æ©Ÿå™¨äººå¤–é€ä»»å‹™çš„ Function Calling ç¯„ä¾‹ï¼š\n",
    "- **baseDrink**: americano, latte, oat_latte, milk\n",
    "- **floor**: 1~11 æ¨“\n",
    "- **temperature**: hot, iced\n",
    "- **addons**: extra_espresso, paper_cup\n",
    "- **quantity**: æ•¸é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "521bdad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š è³‡æ–™é›†å¤§å°: 699 ç­†\n",
      "\n",
      "\u001b[1mè³‡æ–™é›†ç¯„ä¾‹:\u001b[0m\n",
      "{\n",
      "  \"metadata\": \"train\",\n",
      "  \"tools\": [\n",
      "    {\n",
      "      \"type\": \"function\",\n",
      "      \"function\": {\n",
      "        \"name\": \"create_coffee_robot_mission\",\n",
      "        \"description\": \"å»ºç«‹å’–å•¡æ©Ÿå™¨äººå¤–é€ä»»å‹™ã€‚è² è²¬é©—è­‰è¨‚å–®å…§å®¹ï¼Œä¸¦ç”¢ç”Ÿæ©Ÿå™¨äººä»»å‹™æŒ‡ä»¤ (Mock Nuwa Payload)ã€‚\",\n",
      "        \"parameters\": {\n",
      "          \"type\": \"object\",\n",
      "          \"properties\": {\n",
      "            \"baseDrink\": {\n",
      "              \"type\": \"string\",\n",
      "              \"enum\": [\n",
      "                \"americano\",\n",
      "                \"latte\",\n",
      "                \"oat_latte\",\n",
      "                \"milk\"\n",
      "              ],\n",
      "              \"description\": \"åŸºç¤é£²å“ä»£è™Ÿï¼Œåªèƒ½æ˜¯: americano, latte, oat_latte, milk\"\n",
      "            },\n",
      "            \"floor\": {\n",
      "              \"type\": \"integer\",\n",
      "              \"description\": \"é€é”æ¨“å±¤ï¼Œå¿…é ˆä»‹æ–¼ 1 åˆ° 11 ä¹‹é–“\"\n",
      "            },\n",
      "            \"addons\": {\n",
      "              \"type\": \"array\",\n",
      "              \"items\": {\n",
      "                \"type\": \"string\",\n",
      "                \"enum\": [\n",
      "                  \"extra_espresso\",\n",
      "                  \"paper_cup\"\n",
      "                ]\n",
      "              },\n",
      "              \"description\": \"åŠ è³¼é …ç›®æ¸…å–®\"\n",
      "            },\n",
      "            \"quantity\": {\n",
      "              \"type\": \"integer\",\n",
      "              \"description\": \"æ•¸é‡ï¼Œé è¨­ç‚º 1\"\n",
      "            },\n",
      "            \"temperature\": {\n",
      "              \"type\": \"string\",\n",
      "              \"enum\": [\n",
      "                \"hot\",\n",
      "                \"iced\"\n",
      "              ],\n",
      "              \"description\": \"æº«åº¦ï¼Œåªèƒ½æ˜¯ hot æˆ– icedï¼Œé è¨­ç‚º hot\"\n",
      "            }\n",
      "          },\n",
      "          \"required\": [\n",
      "            \"baseDrink\",\n",
      "            \"floor\"\n",
      "          ]\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"å…©æ¯ç¾å¼å»2æ¨“\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"tool_calls\": [\n",
      "        {\n",
      "          \"id\": \"call_b63cfac0\",\n",
      "          \"type\": \"function\",\n",
      "          \"function\": {\n",
      "            \"name\": \"create_coffee_robot_mission\",\n",
      "            \"arguments\": \"{\\\"baseDrink\\\": \\\"americano\\\", \\\"floor\\\": 2, \\\"temperature\\\": \\\"hot\\\", \\\"quantity\\\": 2, \\\"addons\\\": []}\"\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from random import randint\n",
    "from datasets import Dataset\n",
    "\n",
    "# è¼‰å…¥æœ¬åœ° JSONL è³‡æ–™é›†\n",
    "dataset_path = \"ollama_mcp_dataset.jsonl\"\n",
    "\n",
    "data_list = []\n",
    "with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data_list.append(json.loads(line.strip()))\n",
    "\n",
    "# è½‰æ›ç‚º Hugging Face Dataset\n",
    "dataset = Dataset.from_list([{\"text\": json.dumps(item, ensure_ascii=False)} for item in data_list])\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "print(f\"ğŸ“Š è³‡æ–™é›†å¤§å°: {len(dataset)} ç­†\")\n",
    "print(f\"\\n\\033[1mè³‡æ–™é›†ç¯„ä¾‹:\\033[0m\")\n",
    "print(json.dumps(json.loads(dataset[randint(0, len(dataset) - 1)]['text']), indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72dfa0b",
   "metadata": {},
   "source": [
    "## 5. è³‡æ–™é›†æ ¼å¼è½‰æ›\n",
    "\n",
    "å°‡è³‡æ–™é›†è½‰æ›ç‚º TRL SFTTrainer æ‰€éœ€çš„ Prompt-Completion æ ¼å¼ï¼š\n",
    "- `prompt`: ä¸è¨“ç·´çš„éƒ¨åˆ†ï¼ˆä½¿ç”¨è€…è¼¸å…¥ + å·¥å…·å®šç¾©ï¼‰\n",
    "- `completion`: è¦è¨“ç·´çš„éƒ¨åˆ†ï¼ˆæ¨¡å‹å›æ‡‰ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b774a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ è½‰æ›è³‡æ–™é›†æ ¼å¼ä¸­...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc1fefd9f0841f3807e81ce4c71af66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/699 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… è³‡æ–™é›†æ ¼å¼è½‰æ›å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "def apply_format(sample):\n",
    "    \"\"\"å°‡è³‡æ–™é›†æ ¼å¼è½‰æ›ç‚º prompt-completion æ ¼å¼\"\"\"\n",
    "    template_inputs = json.loads(sample['text'])\n",
    "    \n",
    "    # å®Œæ•´çš„å°è©±ï¼ˆåŒ…å« assistant å›æ‡‰ï¼‰\n",
    "    prompt_and_completion = tokenizer.apply_chat_template(\n",
    "        template_inputs['messages'],\n",
    "        tools=template_inputs['tools'],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    \n",
    "    # åªæœ‰ prompt éƒ¨åˆ†ï¼ˆä¸å« assistant å›æ‡‰ï¼‰\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        template_inputs['messages'][:-1],\n",
    "        tools=template_inputs['tools'],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # è¨ˆç®— completionï¼ˆå®Œæ•´å°è©± - promptï¼‰\n",
    "    completion = prompt_and_completion[len(prompt):]\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"completion\": completion,\n",
    "        \"split\": template_inputs.get(\"metadata\", \"train\"),\n",
    "    }\n",
    "\n",
    "# è™•ç†è³‡æ–™é›†\n",
    "print(\"ğŸ”„ è½‰æ›è³‡æ–™é›†æ ¼å¼ä¸­...\")\n",
    "processed_dataset = dataset.map(apply_format)\n",
    "print(\"âœ… è³‡æ–™é›†æ ¼å¼è½‰æ›å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fdb336e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mè™•ç†å¾Œçš„è³‡æ–™é›†ç¯„ä¾‹ï¼š\u001b[0m\n",
      "{\n",
      "  \"text\": \"{\\\"metadata\\\": \\\"train\\\", \\\"tools\\\": [{\\\"type\\\": \\\"function\\\", \\\"function\\\": {\\\"name\\\": \\\"create_coffee_robot_mission\\\", \\\"description\\\": \\\"å»ºç«‹å’–å•¡æ©Ÿå™¨äººå¤–é€ä»»å‹™ã€‚è² è²¬é©—è­‰è¨‚å–®å…§å®¹ï¼Œä¸¦ç”¢ç”Ÿæ©Ÿå™¨äººä»»å‹™æŒ‡ä»¤ (Mock Nuwa Payload)ã€‚\\\", \\\"parameters\\\": {\\\"type\\\": \\\"object\\\", \\\"properties\\\": {\\\"baseDrink\\\": {\\\"type\\\": \\\"string\\\", \\\"enum\\\": [\\\"americano\\\", \\\"latte\\\", \\\"oat_latte\\\", \\\"milk\\\"], \\\"description\\\": \\\"åŸºç¤é£²å“ä»£è™Ÿï¼Œåªèƒ½æ˜¯: americano, latte, oat_latte, milk\\\"}, \\\"floor\\\": {\\\"type\\\": \\\"integer\\\", \\\"description\\\": \\\"é€é”æ¨“å±¤ï¼Œå¿…é ˆä»‹æ–¼ 1 åˆ° 11 ä¹‹é–“\\\"}, \\\"addons\\\": {\\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\", \\\"enum\\\": [\\\"extra_espresso\\\", \\\"paper_cup\\\"]}, \\\"description\\\": \\\"åŠ è³¼é …ç›®æ¸…å–®\\\"}, \\\"quantity\\\": {\\\"type\\\": \\\"integer\\\", \\\"description\\\": \\\"æ•¸é‡ï¼Œé è¨­ç‚º 1\\\"}, \\\"temperature\\\": {\\\"type\\\": \\\"string\\\", \\\"enum\\\": [\\\"hot\\\", \\\"iced\\\"], \\\"description\\\": \\\"æº«åº¦ï¼Œåªèƒ½æ˜¯ hot æˆ– icedï¼Œé è¨­ç‚º hot\\\"}}, \\\"required\\\": [\\\"baseDrink\\\", \\\"floor\\\"]}}}], \\\"messages\\\": [{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"æˆ‘æƒ³è¦ä¸€æ¯åŠ ç´™æ¯çš„æ‹¿éµï¼Œé€åˆ°2æ¨“\\\"}, {\\\"role\\\": \\\"assistant\\\", \\\"tool_calls\\\": [{\\\"id\\\": \\\"call_b7ed4749\\\", \\\"type\\\": \\\"function\\\", \\\"function\\\": {\\\"name\\\": \\\"create_coffee_robot_mission\\\", \\\"arguments\\\": \\\"{\\\\\\\"baseDrink\\\\\\\": \\\\\\\"latte\\\\\\\", \\\\\\\"floor\\\\\\\": 2, \\\\\\\"temperature\\\\\\\": \\\\\\\"hot\\\\\\\", \\\\\\\"quantity\\\\\\\": 1, \\\\\\\"addons\\\\\\\": [\\\\\\\"paper_cup\\\\\\\"]}\\\"}}]}]}\",\n",
      "  \"prompt\": \"<bos><start_of_turn>developer\\n<start_function_declaration>declaration:create_coffee_robot_mission{description:<escape>å»ºç«‹å’–å•¡æ©Ÿå™¨äººå¤–é€ä»»å‹™ã€‚è² è²¬é©—è­‰è¨‚å–®å…§å®¹ï¼Œä¸¦ç”¢ç”Ÿæ©Ÿå™¨äººä»»å‹™æŒ‡ä»¤ (Mock Nuwa Payload)ã€‚<escape>,parameters:{properties:{addons:{description:<escape>åŠ è³¼é …ç›®æ¸…å–®<escape>,items:{enum:[<escape>extra_espresso<escape>,<escape>paper_cup<escape>],type:<escape>STRING<escape>},type:<escape>ARRAY<escape>},baseDrink:{description:<escape>åŸºç¤é£²å“ä»£è™Ÿï¼Œåªèƒ½æ˜¯: americano, latte, oat_latte, milk<escape>,enum:[<escape>americano<escape>,<escape>latte<escape>,<escape>oat_latte<escape>,<escape>milk<escape>],type:<escape>STRING<escape>},floor:{description:<escape>é€é”æ¨“å±¤ï¼Œå¿…é ˆä»‹æ–¼ 1 åˆ° 11 ä¹‹é–“<escape>,type:<escape>INTEGER<escape>},quantity:{description:<escape>æ•¸é‡ï¼Œé è¨­ç‚º 1<escape>,type:<escape>INTEGER<escape>},temperature:{description:<escape>æº«åº¦ï¼Œåªèƒ½æ˜¯ hot æˆ– icedï¼Œé è¨­ç‚º hot<escape>,enum:[<escape>hot<escape>,<escape>iced<escape>],type:<escape>STRING<escape>}},required:[<escape>baseDrink<escape>,<escape>floor<escape>],type:<escape>OBJECT<escape>}}<end_function_declaration><end_of_turn>\\n<start_of_turn>user\\næˆ‘æƒ³è¦ä¸€æ¯åŠ ç´™æ¯çš„æ‹¿éµï¼Œé€åˆ°2æ¨“<end_of_turn>\\n<start_of_turn>model\\n\",\n",
      "  \"completion\": \"<start_function_call>call:create_coffee_robot_mission{                    {\\\"baseDrink\\\": \\\"latte\\\", \\\"floor\\\": 2, \\\"temperature\\\": \\\"hot\\\", \\\"quantity\\\": 1, \\\"addons\\\": [\\\"paper_cup\\\"]}}<end_function_call><start_function_response>\",\n",
      "  \"split\": \"train\"\n",
      "}\n",
      "\n",
      "\u001b[1mæœ€é•·ç¯„ä¾‹é•·åº¦: 1357 å­—å…ƒ, 336 tokens\u001b[0m\n",
      "\u001b[1mä½¿ç”¨ max_token_count: 436\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# æª¢è¦–è™•ç†å¾Œçš„è³‡æ–™é›†ç¯„ä¾‹\n",
    "print(\"\\033[1mè™•ç†å¾Œçš„è³‡æ–™é›†ç¯„ä¾‹ï¼š\\033[0m\")\n",
    "sample = processed_dataset[randint(0, len(processed_dataset) - 1)]\n",
    "print(json.dumps(sample, indent=2, ensure_ascii=False))\n",
    "\n",
    "# è¨ˆç®—æœ€é•·ç¯„ä¾‹çš„ token æ•¸\n",
    "longest_example = max(processed_dataset, key=lambda x: len(x['prompt'] + x['completion']))\n",
    "longest_token_count = len(tokenizer.tokenize(longest_example['prompt'] + longest_example['completion']))\n",
    "\n",
    "print(f\"\\n\\033[1mæœ€é•·ç¯„ä¾‹é•·åº¦: {len(longest_example['prompt'] + longest_example['completion'])} å­—å…ƒ, {longest_token_count} tokens\\033[0m\")\n",
    "\n",
    "# è¨­å®š max_token_countï¼ˆåŠ ä¸Šç·©è¡ï¼‰\n",
    "max_token_count = longest_token_count + 100\n",
    "print(f\"\\033[1mä½¿ç”¨ max_token_count: {max_token_count}\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "504e4421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5e280ac3d146fcad44449b2fc28163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/699 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddaf0220236b48e78fc5586f01631b86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/699 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š è¨“ç·´é›†: 625 ç­†\n",
      "ğŸ“Š é©—è­‰é›†: 74 ç­†\n"
     ]
    }
   ],
   "source": [
    "# åˆ†å‰²è¨“ç·´é›†èˆ‡é©—è­‰é›†\n",
    "train_dataset = processed_dataset.filter(lambda x: x['split'] == 'train')\n",
    "eval_dataset = processed_dataset.filter(lambda x: x['split'] == 'eval')\n",
    "\n",
    "print(f\"ğŸ“Š è¨“ç·´é›†: {len(train_dataset)} ç­†\")\n",
    "print(f\"ğŸ“Š é©—è­‰é›†: {len(eval_dataset)} ç­†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782297ca",
   "metadata": {},
   "source": [
    "## 6. æ¸¬è©¦åŸºç¤æ¨¡å‹ï¼ˆå¾®èª¿å‰ï¼‰\n",
    "\n",
    "å…ˆæ¸¬è©¦ FunctionGemma åŸºç¤æ¨¡å‹åœ¨å’–å•¡æ©Ÿå™¨äººä»»å‹™ä¸Šçš„è¡¨ç¾ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3007803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mPrompt:\u001b[0m å¹«æˆ‘é€ä¸€æ¯ç†±ç¾å¼åˆ°äº”æ¨“\n",
      "\n",
      "\u001b[1måŸºç¤æ¨¡å‹è¼¸å‡º:\u001b[0m I can help with that. Could you please specify the exact floor number for the delivery to the 5th floor?\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# æ¸…ç†ä¹‹å‰å¯èƒ½å­˜åœ¨çš„ pipeline\n",
    "gc.collect()\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "# å»ºç«‹æ¨è«– pipelineï¼ˆä½¿ç”¨è¼ƒå°‘è¨˜æ†¶é«”çš„è¨­å®šï¼‰\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=gemma_model, \n",
    "    tokenizer=tokenizer,\n",
    "    device=\"mps\",\n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "\n",
    "# æ¸¬è©¦æç¤º\n",
    "user_prompt = \"å¹«æˆ‘é€ä¸€æ¯ç†±ç¾å¼åˆ°äº”æ¨“\"\n",
    "\n",
    "# å¾è³‡æ–™é›†å–å¾—å·¥å…·å®šç¾©\n",
    "tools = json.loads(dataset[0]['text'])['tools']\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tools=tools,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(f\"\\n\\033[1mPrompt:\\033[0m {user_prompt}\")\n",
    "output = pipe(prompt, max_new_tokens=max_token_count)\n",
    "model_output = output[0]['generated_text'][len(prompt):].strip()\n",
    "print(f\"\\n\\033[1måŸºç¤æ¨¡å‹è¼¸å‡º:\\033[0m {model_output}\")\n",
    "\n",
    "# æ¸¬è©¦å®Œé‡‹æ”¾ pipeline\n",
    "del pipe\n",
    "gc.collect()\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "print(\"\\nâœ… æ¸¬è©¦å®Œæˆï¼Œå·²é‡‹æ”¾è¨˜æ†¶é«”\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e76480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mé æœŸè¼¸å‡º:\u001b[0m <start_function_call>call:create_coffee_robot_mission{                    {\"baseDrink\": \"milk\", \"floor\": 1, \"temperature\": \"hot\", \"quantity\": 1, \"addons\": [\"paper_cup\"]}}<end_function_call><start_function_response>\n",
      "\n",
      "\u001b[1må¯¦éš›è¼¸å‡º:\u001b[0m <start_function_call>call:create_coffee_robot_mission{addons:[<escape>extra_espresso<escape>],baseDrink:[<escape>americano<escape>,<escape>latte<escape>,<escape>oat_latte<escape>,<escape>milk<escape>],floor:1,quantity:1,temperature:<escape>hot<escape>}<end_function_call><start_function_response>\n"
     ]
    }
   ],
   "source": [
    "# è·³éé€™å€‹æ¸¬è©¦ä»¥ç¯€çœè¨˜æ†¶é«”ï¼Œè¨“ç·´å®Œå†æ¸¬è©¦\n",
    "# å¾è¨“ç·´é›†ä¸­é¸å–ä¸€ç­†è³‡æ–™æ¸¬è©¦\n",
    "# rand_idx = randint(0, len(train_dataset) - 1)\n",
    "# test_sample = train_dataset[rand_idx]\n",
    "# ...\n",
    "\n",
    "print(\"â­ï¸ è·³éé¡å¤–æ¸¬è©¦ä»¥ä¿ç•™è¨˜æ†¶é«”çµ¦è¨“ç·´ä½¿ç”¨\")\n",
    "print(\"ğŸ’¡ è¨“ç·´å®Œæˆå¾Œæœƒé€²è¡Œå®Œæ•´æ¸¬è©¦\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c9fb89",
   "metadata": {},
   "source": [
    "## 7. é…ç½®å¾®èª¿è¨“ç·´\n",
    "\n",
    "ä½¿ç”¨ TRL çš„ SFTConfig é…ç½®è¨“ç·´åƒæ•¸ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4041f1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… è¨“ç·´é…ç½®å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "from transformers import AutoModelForCausalLM\n",
    "from trl import SFTConfig\n",
    "\n",
    "# æ¸…ç†è¨˜æ†¶é«”\n",
    "gc.collect()\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "# è¼¸å‡ºç›®éŒ„\n",
    "output_dir = \"./coffee-robot-functiongemma\"\n",
    "\n",
    "# é‡æ–°è¼‰å…¥ tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(gemma_model)\n",
    "\n",
    "# ======== M4 Pro 48GB å„ªåŒ–é…ç½® ========\n",
    "args = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,                               # è¨“ç·´è¼ªæ•¸\n",
    "    per_device_train_batch_size=1,                    # â¬‡ï¸ é™ä½æ‰¹æ¬¡å¤§å°ä»¥æ¸›å°‘è¨˜æ†¶é«”\n",
    "    per_device_eval_batch_size=1,                     # â¬‡ï¸ é™ä½è©•ä¼°æ‰¹æ¬¡å¤§å°\n",
    "    gradient_accumulation_steps=16,                   # â¬†ï¸ å¢åŠ æ¢¯åº¦ç´¯ç©è£œå„Ÿå°æ‰¹æ¬¡\n",
    "    logging_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,                               # å­¸ç¿’ç‡\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_length=max_token_count,\n",
    "    gradient_checkpointing=True,                      # âœ… ä¿æŒå•Ÿç”¨ä»¥ç¯€çœè¨˜æ†¶é«”\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # æ›´ç¯€çœè¨˜æ†¶é«”\n",
    "    packing=False,\n",
    "    optim=\"adamw_torch\",                              # ğŸ”§ æ”¹ç”¨æ¨™æº– AdamWï¼ˆMPS ç›¸å®¹æ€§æ›´å¥½ï¼‰\n",
    "    bf16=False,                                       # ğŸ”§ MPS ä¸æ”¯æ´ bf16\n",
    "    fp16=False,                                       # ğŸ”§ MPS å° fp16 æ”¯æ´æœ‰é™ï¼Œç”¨ fp32 æ›´ç©©å®š\n",
    "    completion_only_loss=True,                        # åªè¨ˆç®— completion çš„ loss\n",
    "    report_to=\"none\",\n",
    "    dataloader_pin_memory=False,                      # ğŸ”§ MPS ä¸éœ€è¦ pin_memory\n",
    "    dataloader_num_workers=0,                         # ğŸ”§ é¿å…å¤šé€²ç¨‹å•é¡Œ\n",
    "    max_grad_norm=1.0,                                # æ¢¯åº¦è£å‰ªé˜²æ­¢è¨˜æ†¶é«”çˆ†ç‚¸\n",
    ")\n",
    "\n",
    "# è¼‰å…¥åŸºç¤æ¨¡å‹ - é‡å° Apple Silicon å„ªåŒ–\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    gemma_model,\n",
    "    device_map=\"mps\",                                 # ğŸ”§ æ˜ç¢ºæŒ‡å®š MPS\n",
    "    torch_dtype=torch.float32,                        # ğŸ”§ ä½¿ç”¨ fp32 ä»¥ç²å¾—æœ€ä½³ MPS ç›¸å®¹æ€§\n",
    "    attn_implementation='eager',\n",
    "    low_cpu_mem_usage=True,                           # ğŸ”§ æ¸›å°‘ CPU è¨˜æ†¶é«”ä½¿ç”¨\n",
    ")\n",
    "\n",
    "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# å•Ÿç”¨è¨˜æ†¶é«”é«˜æ•ˆè¨­å®š\n",
    "base_model.config.use_cache = False                   # è¨“ç·´æ™‚é—œé–‰ KV cache\n",
    "\n",
    "print(\"âœ… è¨“ç·´é…ç½®å®Œæˆï¼\")\n",
    "print(f\"ğŸ“± Device: {next(base_model.parameters()).device}\")\n",
    "print(f\"ğŸ“Š Effective batch size: {args.per_device_train_batch_size * args.gradient_accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93135209",
   "metadata": {},
   "source": [
    "## 8. é–‹å§‹è¨“ç·´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57266c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acecfa2adf784758ac697b140ff38459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/625 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20dad28a27864f5bb0c270aedef7ef74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/625 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d9ec25c4144a10a5a1c7f809876c26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/625 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abcc646255ee4125af4f994efdc940d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/74 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f9c6d925ae4e3f9414c0f335f697cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/74 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "487f185fe7e34c8b8ea9b403a629b540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/74 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1, 'bos_token_id': 2, 'pad_token_id': 0}.\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ é–‹å§‹è¨“ç·´...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [51/60 06:16 < 01:09, 0.13 it/s, Epoch 2.51/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/10 00:02 < 00:16, 0.48 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml-env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# è¨“ç·´å‰æ¸…ç†è¨˜æ†¶é«”\n",
    "gc.collect()\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "# å»ºç«‹è¨“ç·´å™¨\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "print(\"ğŸš€ é–‹å§‹è¨“ç·´...\")\n",
    "print(f\"ğŸ“Š è¨“ç·´é›†å¤§å°: {len(train_dataset)}\")\n",
    "print(f\"ğŸ“Š é©—è­‰é›†å¤§å°: {len(eval_dataset)}\")\n",
    "\n",
    "# é–‹å§‹è¨“ç·´\n",
    "trainer.train()\n",
    "\n",
    "# è¨“ç·´å®Œæˆå¾Œæ¸…ç†è¨˜æ†¶é«”\n",
    "gc.collect()\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "# å„²å­˜æ¨¡å‹\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"\\nâœ… å¾®èª¿æ¨¡å‹å·²å„²å­˜è‡³: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be252b0",
   "metadata": {},
   "source": [
    "## 9. è¦–è¦ºåŒ–è¨“ç·´çµæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fa7969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# å–å¾—è¨“ç·´è¨˜éŒ„\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# æå– loss\n",
    "train_losses = [log[\"loss\"] for log in log_history if \"loss\" in log]\n",
    "epoch_train = [log[\"epoch\"] for log in log_history if \"loss\" in log]\n",
    "eval_losses = [log[\"eval_loss\"] for log in log_history if \"eval_loss\" in log]\n",
    "epoch_eval = [log[\"epoch\"] for log in log_history if \"eval_loss\" in log]\n",
    "\n",
    "# ç¹ªè£½åœ–è¡¨\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epoch_train, train_losses, label=\"Training Loss\", marker='o', markersize=3)\n",
    "plt.plot(epoch_eval, eval_losses, label=\"Validation Loss\", marker='s', markersize=3)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Coffee Robot MCP - Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f840e8",
   "metadata": {},
   "source": [
    "## 10. æ¸¬è©¦å¾®èª¿å¾Œçš„æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207349af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# è¼‰å…¥å¾®èª¿å¾Œçš„æ¨¡å‹\n",
    "trained_model = AutoModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "# å»ºç«‹ pipeline\n",
    "pipe_trained = pipeline(\"text-generation\", model=trained_model, tokenizer=tokenizer)\n",
    "pipe_base = pipeline(\"text-generation\", model=gemma_model, device_map=\"auto\")\n",
    "\n",
    "# å¾è³‡æ–™é›†å–å¾—å·¥å…·å®šç¾©\n",
    "tools = json.loads(dataset[0]['text'])['tools']\n",
    "\n",
    "print(\"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eadbd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸¬è©¦å¤šå€‹æç¤º\n",
    "test_prompts = [\n",
    "    \"å¹«æˆ‘é€ä¸€æ¯ç†±ç¾å¼åˆ°äº”æ¨“\",\n",
    "    \"æˆ‘è¦ä¸‰æ¯å†°æ‹¿éµé€åˆ° 11 æ¨“ï¼Œéƒ½è¦åŠ æ¿ƒç¸®\",\n",
    "    \"å¥½ç´¯å–”ï¼Œä¾†æ¯ç‡•éº¥å¥¶æ‹¿éµæç¥ï¼Œæˆ‘åœ¨ä¸ƒæ¨“\",\n",
    "    \"ä¸€æ¯é®®ä¹³ï¼Œç”¨ç´™æ¯è£ï¼Œé€åˆ°ä¸‰æ¨“\",\n",
    "]\n",
    "\n",
    "for user_prompt in test_prompts:\n",
    "    messages = [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tools=tools,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    output_trained = pipe_trained(prompt, max_new_tokens=max_token_count)\n",
    "    output_base = pipe_base(prompt, max_new_tokens=max_token_count)\n",
    "    \n",
    "    model_output_trained = output_trained[0]['generated_text'][len(prompt):].strip()\n",
    "    model_output_base = output_base[0]['generated_text'][len(prompt):].strip()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"\\033[1mä½¿ç”¨è€…è¼¸å…¥:\\033[0m {user_prompt}\")\n",
    "    print(f\"\\033[1må¾®èª¿æ¨¡å‹è¼¸å‡º:\\033[0m {model_output_trained}\")\n",
    "    print(f\"\\033[1måŸºç¤æ¨¡å‹è¼¸å‡º:\\033[0m {model_output_base}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55c4e99",
   "metadata": {},
   "source": [
    "## 11. è©•ä¼°æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b63c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_function_call(model_output):\n",
    "    \"\"\"è§£æ Function Call è¼¸å‡º\"\"\"\n",
    "    results = []\n",
    "    call_pattern = r\"<start_function_call>(.*?)<end_function_call>\"\n",
    "    raw_calls = re.findall(call_pattern, model_output, re.DOTALL)\n",
    "    \n",
    "    for raw_call in raw_calls:\n",
    "        if not raw_call.strip().startswith(\"call:\"):\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            pre_brace, args_segment = raw_call.split(\"{\", 1)\n",
    "            function_name = pre_brace.replace(\"call:\", \"\").strip()\n",
    "            \n",
    "            args_content = args_segment.strip()\n",
    "            if args_content.endswith(\"}\"):\n",
    "                args_content = args_content[:-1]\n",
    "            \n",
    "            arguments = {}\n",
    "            arg_pattern = r\"(?P<key>[^:,]*?):<escape>(?P<value>.*?)<escape>\"\n",
    "            arg_matches = re.finditer(arg_pattern, args_content, re.DOTALL)\n",
    "            \n",
    "            for match in arg_matches:\n",
    "                key = match.group(\"key\").strip()\n",
    "                value = match.group(\"value\")\n",
    "                arguments[key] = value\n",
    "            \n",
    "            results.append({\n",
    "                \"function\": {\n",
    "                    \"name\": function_name,\n",
    "                    \"arguments\": arguments\n",
    "                }\n",
    "            })\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "def evaluate_model(dataset, pipe, desc=\"Evaluating\"):\n",
    "    \"\"\"è©•ä¼°æ¨¡å‹åœ¨è³‡æ–™é›†ä¸Šçš„è¡¨ç¾\"\"\"\n",
    "    logs = []\n",
    "    \n",
    "    for i in tqdm(range(len(dataset)), desc=desc):\n",
    "        orig_data = dataset[i]['text']\n",
    "        messages = json.loads(orig_data)['messages']\n",
    "        user_message = messages[0]['content']\n",
    "        assistant_message = messages[1]\n",
    "        input_prompt = dataset[i]['prompt']\n",
    "        \n",
    "        output = pipe(input_prompt, max_new_tokens=max_token_count)\n",
    "        model_output_only = output[0]['generated_text'][len(input_prompt):].strip()\n",
    "        \n",
    "        logs.append({\n",
    "            \"user\": user_message,\n",
    "            \"target_fc\": assistant_message.get('tool_calls', []),\n",
    "            \"output_fc\": extract_function_call(model_output_only),\n",
    "        })\n",
    "    \n",
    "    return logs\n",
    "\n",
    "def calculate_score(logs):\n",
    "    \"\"\"è¨ˆç®—è©•ä¼°åˆ†æ•¸\"\"\"\n",
    "    df = pd.DataFrame.from_records(logs)\n",
    "    \n",
    "    df['target_names'] = df['target_fc'].apply(lambda x: [fc['function']['name'] for fc in x] if x else [])\n",
    "    df['output_names'] = df['output_fc'].apply(lambda x: [fc['function']['name'] for fc in x] if x else [])\n",
    "    df['correct_names'] = df['target_names'] == df['output_names']\n",
    "    \n",
    "    return df, df['correct_names'].mean()\n",
    "\n",
    "print(\"âœ… è©•ä¼°å‡½æ•¸å®šç¾©å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396028a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è©•ä¼°åŸºç¤æ¨¡å‹\n",
    "print(\"ğŸ“Š è©•ä¼°åŸºç¤æ¨¡å‹...\")\n",
    "base_logs = evaluate_model(\n",
    "    eval_dataset,\n",
    "    pipeline(\"text-generation\", model=gemma_model, device_map=\"auto\", temperature=0.001),\n",
    "    desc=\"Base Model\"\n",
    ")\n",
    "base_df, base_score = calculate_score(base_logs)\n",
    "\n",
    "print(f\"\\n\\033[1måŸºç¤æ¨¡å‹æº–ç¢ºç‡:\\033[0m {base_score:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13727c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è©•ä¼°å¾®èª¿æ¨¡å‹\n",
    "print(\"ğŸ“Š è©•ä¼°å¾®èª¿æ¨¡å‹...\")\n",
    "trained_logs = evaluate_model(\n",
    "    eval_dataset,\n",
    "    pipeline(\"text-generation\", model=trained_model, tokenizer=tokenizer, temperature=0.001),\n",
    "    desc=\"Fine-tuned Model\"\n",
    ")\n",
    "trained_df, trained_score = calculate_score(trained_logs)\n",
    "\n",
    "print(f\"\\n\\033[1må¾®èª¿æ¨¡å‹æº–ç¢ºç‡:\\033[0m {trained_score:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e31f2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¯”è¼ƒçµæœ\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸ“Š æ¨¡å‹æ¯”è¼ƒçµæœ\")\n",
    "print(\"=\"*50)\n",
    "print(f\"åŸºç¤æ¨¡å‹æº–ç¢ºç‡:   {base_score:.2%}\")\n",
    "print(f\"å¾®èª¿æ¨¡å‹æº–ç¢ºç‡:   {trained_score:.2%}\")\n",
    "print(f\"æº–ç¢ºç‡æå‡:       {(trained_score - base_score):.2%}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# å„²å­˜è©•ä¼°çµæœ\n",
    "base_df.to_json('eval_base_model.json', orient='records', force_ascii=False)\n",
    "trained_df.to_json('eval_trained_model.json', orient='records', force_ascii=False)\n",
    "print(\"\\nâœ… è©•ä¼°çµæœå·²å„²å­˜ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1836d047",
   "metadata": {},
   "source": [
    "## 12. ä¸Šå‚³æ¨¡å‹è‡³ Hugging Face Hubï¼ˆå¯é¸ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c37d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import ModelCard, whoami\n",
    "\n",
    "# è¨­å®šæ¨¡å‹åç¨±\n",
    "model_name = \"coffee-robot-mcp\"\n",
    "\n",
    "# å–å¾—ä½¿ç”¨è€…åç¨±\n",
    "username = whoami()['name']\n",
    "hf_repo_id = f\"{username}/functiongemma-270m-it-{model_name}\"\n",
    "\n",
    "# ä¸Šå‚³æ¨¡å‹\n",
    "print(f\"ğŸ“¤ ä¸Šå‚³æ¨¡å‹è‡³: {hf_repo_id}\")\n",
    "repo_url = trained_model.push_to_hub(hf_repo_id, create_repo=True, commit_message=\"Upload coffee robot MCP model\")\n",
    "tokenizer.push_to_hub(hf_repo_id)\n",
    "\n",
    "# å»ºç«‹ Model Card\n",
    "card_content = f\"\"\"\n",
    "---\n",
    "base_model: {gemma_model}\n",
    "tags:\n",
    "- function-calling\n",
    "- coffee-robot\n",
    "- mcp\n",
    "- gemma\n",
    "language:\n",
    "- zh\n",
    "---\n",
    "\n",
    "# Coffee Robot MCP - FunctionGemma 270M\n",
    "\n",
    "åŸºæ–¼ `{gemma_model}` å¾®èª¿çš„å’–å•¡æ©Ÿå™¨äººå¤–é€ä»»å‹™æ¨¡å‹ã€‚\n",
    "\n",
    "## åŠŸèƒ½\n",
    "- é£²å“é¸æ“‡: americano, latte, oat_latte, milk\n",
    "- æ¨“å±¤é…é€: 1-11 æ¨“\n",
    "- æº«åº¦é¸æ“‡: hot, iced\n",
    "- åŠ è³¼é¸é …: extra_espresso, paper_cup\n",
    "\n",
    "## ä½¿ç”¨æ–¹å¼\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"{hf_repo_id}\")\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "card = ModelCard(card_content)\n",
    "card.push_to_hub(hf_repo_id)\n",
    "\n",
    "print(f\"\\nâœ… æ¨¡å‹å·²ä¸Šå‚³è‡³: {repo_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344fad82",
   "metadata": {},
   "source": [
    "## 13. ç¸½çµ\n",
    "\n",
    "ğŸ‰ **æ­å–œï¼ä½ å·²ç¶“å®Œæˆ FunctionGemma çš„å¾®èª¿ï¼**\n",
    "\n",
    "### å®Œæˆçš„å·¥ä½œ\n",
    "1. âœ… è¼‰å…¥ä¸¦è™•ç† `ollama_mcp_dataset.jsonl` è³‡æ–™é›†\n",
    "2. âœ… å¾®èª¿ FunctionGemma 270M æ¨¡å‹\n",
    "3. âœ… è©•ä¼°æ¨¡å‹è¡¨ç¾\n",
    "4. âœ… å„²å­˜å¾®èª¿å¾Œçš„æ¨¡å‹\n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "- ä½¿ç”¨æ›´å¤šè³‡æ–™ç¹¼çºŒè¨“ç·´\n",
    "- èª¿æ•´è¶…åƒæ•¸å„ªåŒ–æ•ˆæœ\n",
    "- éƒ¨ç½²æ¨¡å‹è‡³ç”Ÿç”¢ç’°å¢ƒ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
