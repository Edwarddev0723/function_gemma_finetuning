import json
import random
import uuid
import requests  # pip install requests
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm  # pip install tqdm

# ==========================================
# 1. é…ç½®è¨­å®š
# ==========================================
# è«‹ç¢ºä¿ä½ çš„ Ollama æœå‹™æ­£åœ¨é‹è¡Œ (ollama serve)
OLLAMA_API_URL = "http://localhost:11434/api/chat"
MODEL_NAME = "gpt-oss:20b"  # è«‹æ ¹æ“šä½ æœ¬åœ°æœ‰çš„æ¨¡å‹ä¿®æ”¹ï¼Œä¾‹å¦‚ "mistral", "llama3", "gemma2"
OUTPUT_FILE = "ollama_mcp_dataset.jsonl"
TOTAL_SAMPLES = 1000   # æƒ³è¦ç”Ÿæˆçš„ç¸½ç­†æ•¸
EVAL_RATIO = 0.1     # é©—è­‰é›†æ¯”ä¾‹
BATCH_SIZE = 10      # æ¯æ¬¡ç”Ÿæˆçš„æ¨£æœ¬æ•¸ï¼ˆå¢å¤§ä»¥æ¸›å°‘è«‹æ±‚æ¬¡æ•¸ï¼‰
MAX_WORKERS = 4      # ä¸¦è¡Œè«‹æ±‚æ•¸é‡

# ==========================================
# 2. MCP æ¥­å‹™é‚è¼¯èˆ‡å¸¸æ•¸ (é©—è­‰ç”¨)
# ==========================================
BASE_DRINKS = {
    "americano": "ç¾å¼",
    "latte": "æ‹¿éµ",
    "oat_latte": "ç‡•éº¥å¥¶æ‹¿éµ",
    "milk": "é®®ä¹³"
}
ADDONS = {
    "extra_espresso": "åŠ è³¼ä¸€ä»½æ¿ƒç¸®å’–å•¡",
    "paper_cup": "ç´™æ¯"
}

# å·¥å…·å®šç¾© Schema
TOOL_SCHEMA = {
    "type": "function",
    "function": {
        "name": "create_coffee_robot_mission",
        "description": "å»ºç«‹å’–å•¡æ©Ÿå™¨äººå¤–é€ä»»å‹™ã€‚è² è²¬é©—è­‰è¨‚å–®å…§å®¹ï¼Œä¸¦ç”¢ç”Ÿæ©Ÿå™¨äººä»»å‹™æŒ‡ä»¤ (Mock Nuwa Payload)ã€‚",
        "parameters": {
            "type": "object",
            "properties": {
                "baseDrink": {
                    "type": "string",
                    "enum": list(BASE_DRINKS.keys()),
                    "description": "åŸºç¤é£²å“ä»£è™Ÿï¼Œåªèƒ½æ˜¯: " + ", ".join(BASE_DRINKS.keys())
                },
                "floor": {
                    "type": "integer",
                    "description": "é€é”æ¨“å±¤ï¼Œå¿…é ˆä»‹æ–¼ 1 åˆ° 11 ä¹‹é–“"
                },
                "addons": {
                    "type": "array",
                    "items": {
                        "type": "string",
                        "enum": list(ADDONS.keys())
                    },
                    "description": "åŠ è³¼é …ç›®æ¸…å–®"
                },
                "quantity": {
                    "type": "integer",
                    "description": "æ•¸é‡ï¼Œé è¨­ç‚º 1"
                },
                "temperature": {
                    "type": "string",
                    "enum": ["hot", "iced"],
                    "description": "æº«åº¦ï¼Œåªèƒ½æ˜¯ hot æˆ– icedï¼Œé è¨­ç‚º hot"
                }
            },
            "required": ["baseDrink", "floor"]
        }
    }
}

# ==========================================
# 3. æ ¸å¿ƒå‡½æ•¸ï¼šä½¿ç”¨ Ollama ç”Ÿæˆè‡ªç„¶èªè¨€
# ==========================================

def query_ollama(prompt: str, system_prompt: str, timeout: int = 120) -> str:
    payload = {
        "model": MODEL_NAME,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt}
        ],
        "stream": False,
        "format": "json",  # å¼·åˆ¶è®“ Ollama å›å‚³ JSON æ ¼å¼
        "options": {
            "num_predict": 2048,  # é™åˆ¶ç”Ÿæˆé•·åº¦ä»¥åŠ é€Ÿ
            "temperature": 0.8,   # é©åº¦å¤šæ¨£æ€§
        }
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, timeout=timeout)
        response.raise_for_status()
        return response.json()["message"]["content"]
    except requests.exceptions.Timeout:
        return ""
    except Exception as e:
        print(f"\nOllama API Error: {e}")
        return ""

# ç³»çµ±æç¤ºè©ï¼ˆå…¨åŸŸå¸¸æ•¸ï¼Œé¿å…é‡è¤‡å­—ä¸²ï¼‰
SYSTEM_PROMPT = """ä½ æ˜¯ä¸€å€‹è³‡æ–™ç”ŸæˆåŠ©æ‰‹ã€‚è«‹ç”ŸæˆçœŸå¯¦ã€å£èªåŒ–çš„ä½¿ç”¨è€…é»é¤æŒ‡ä»¤ï¼Œä¸¦å°æ‡‰åˆ°æ­£ç¢ºçš„ JSON åƒæ•¸ã€‚

æ¥­å‹™è¦å‰‡ï¼š
- é£²å“ (baseDrink): americano, latte, oat_latte, milk
- æ¨“å±¤ (floor): 1~11
- æº«åº¦ (temperature): hot, iced
- åŠ è³¼ (addons): extra_espresso, paper_cup
- æ•¸é‡ (quantity): é è¨­ 1

è«‹å›å‚³ä¸€å€‹ JSON Objectï¼ŒåŒ…å«ä¸€å€‹ "data" åˆ—è¡¨ï¼Œåˆ—è¡¨ä¸­çš„æ¯å€‹ç‰©ä»¶æ ¼å¼å¦‚ä¸‹ï¼š
{
  "user_input": "å¹«æˆ‘é€ä¸€æ¯ç†±ç¾å¼å»äº”æ¨“",
  "args": {
    "baseDrink": "americano", 
    "floor": 5, 
    "temperature": "hot", 
    "quantity": 1,
    "addons": []
  }
}

è«‹ç”Ÿæˆå¤šæ¨£åŒ–çš„èªå¥ï¼ŒåŒ…å«ï¼š
1. ç°¡å–®æŒ‡ä»¤ ("ä¸€æ¯æ‹¿éµåˆ°3æ¨“")
2. è¤‡é›œéœ€æ±‚ ("æˆ‘è¦ä¸‰æ¯ç‡•éº¥å¥¶æ‹¿éµï¼Œéƒ½è¦å†°çš„ï¼Œé€åˆ°11æ¨“æœƒè­°å®¤")
3. éš±æ™¦éœ€æ±‚ ("å¥½ç´¯å–”ï¼Œä¾†æ¯åŠ æ¿ƒç¸®çš„ç¾å¼æç¥ï¼Œæˆ‘åœ¨7æ¨“")
"""

def generate_synthetic_data(batch_size: int = 10) -> List[Dict]:
    user_prompt = f"è«‹ç”Ÿæˆ {batch_size} ç­†æ¸¬è©¦è³‡æ–™ã€‚è«‹ç¢ºä¿ JSON æ ¼å¼æ­£ç¢ºä¸”åƒæ•¸ç¬¦åˆæ¥­å‹™è¦å‰‡ã€‚"

    response_text = query_ollama(user_prompt, SYSTEM_PROMPT)

    if not response_text:
        return []

    try:
        data = json.loads(response_text)
        if isinstance(data, list):
            return data
        elif "data" in data:
            return data["data"]
        else:
            return []
    except json.JSONDecodeError:
        return []


def generate_batch_parallel(num_batches: int, batch_size: int) -> List[Dict]:
    """ä¸¦è¡ŒåŸ·è¡Œå¤šå€‹æ‰¹æ¬¡è«‹æ±‚"""
    all_results = []
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(generate_synthetic_data, batch_size) for _ in range(num_batches)]
        
        for future in as_completed(futures):
            try:
                batch = future.result()
                all_results.extend(batch)
            except Exception:
                pass
    
    return all_results

# ==========================================
# 4. é©—è­‰èˆ‡æ ¼å¼è½‰æ›
# ==========================================

def validate_args(args: Dict) -> bool:
    # ç°¡å–®é©—è­‰ç”Ÿæˆçš„åƒæ•¸æ˜¯å¦ç¬¦åˆ MCP è¦å‰‡
    try:
        if args.get("baseDrink") not in BASE_DRINKS: return False
        if not (1 <= args.get("floor", 0) <= 11): return False
        if args.get("temperature") not in ["hot", "iced"]: return False
        if args.get("quantity", 0) < 1: return False
        # æª¢æŸ¥ addons
        for addon in args.get("addons", []):
            if addon not in ADDONS: return False
        return True
    except:
        return False

def create_dataset_entry(user_input: str, args: Dict, split: str) -> Dict:
    # ç¢ºä¿ args æ˜¯ JSON å­—ä¸²
    args_str = json.dumps(args, ensure_ascii=False)

    return {
        "metadata": split,
        "tools": [TOOL_SCHEMA],
        "messages": [
            {
                "role": "user",
                "content": user_input
            },
            {
                "role": "assistant",
                "tool_calls": [
                    {
                        "id": f"call_{uuid.uuid4().hex[:8]}",
                        "type": "function",
                        "function": {
                            "name": "create_coffee_robot_mission",
                            "arguments": args_str
                        }
                    }
                ]
            }
        ]
    }

# ==========================================
# 5. ä¸»ç¨‹å¼åŸ·è¡Œ
# ==========================================

def main():
    print(f"ğŸš€ é–‹å§‹ä½¿ç”¨ Ollama ({MODEL_NAME}) ç”Ÿæˆè³‡æ–™é›†...")
    print(f"   ç›®æ¨™: {TOTAL_SAMPLES} ç­† | æ‰¹æ¬¡å¤§å°: {BATCH_SIZE} | ä¸¦è¡Œæ•¸: {MAX_WORKERS}")
    print("-" * 50)
    
    valid_samples = []
    seen_inputs = set()  # ç”¨æ–¼å»é‡

    # è¨ˆç®—éœ€è¦çš„ç¸½æ‰¹æ¬¡æ•¸ï¼ˆè€ƒæ…®é©—è­‰å¤±æ•—ç‡ï¼Œå¤šè«‹æ±‚ä¸€äº›ï¼‰
    estimated_batches = (TOTAL_SAMPLES * 2) // BATCH_SIZE + 1
    
    # ä½¿ç”¨ tqdm é€²åº¦æ¢
    pbar = tqdm(total=TOTAL_SAMPLES, desc="ç”Ÿæˆæœ‰æ•ˆè³‡æ–™", unit="ç­†", 
                bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]')
    
    max_rounds = 50  # æœ€å¤šåŸ·è¡Œ 50 è¼ªä¸¦è¡Œè«‹æ±‚
    round_count = 0
    
    while len(valid_samples) < TOTAL_SAMPLES and round_count < max_rounds:
        round_count += 1
        
        # è¨ˆç®—é€™ä¸€è¼ªéœ€è¦å¤šå°‘ä¸¦è¡Œè«‹æ±‚
        remaining = TOTAL_SAMPLES - len(valid_samples)
        num_batches = min(MAX_WORKERS, (remaining // BATCH_SIZE) + 1)
        
        # ä¸¦è¡Œç”Ÿæˆè³‡æ–™
        batch_results = generate_batch_parallel(num_batches, BATCH_SIZE)
        
        # é©—è­‰ä¸¦æ·»åŠ æœ‰æ•ˆè³‡æ–™
        for item in batch_results:
            if len(valid_samples) >= TOTAL_SAMPLES:
                break

            user_input = item.get("user_input", "")
            args = item.get("args")

            # é©—è­‰è³‡æ–™æœ‰æ•ˆæ€§ä¸¦å»é‡
            if user_input and args and validate_args(args) and user_input not in seen_inputs:
                seen_inputs.add(user_input)
                split = "eval" if random.random() < EVAL_RATIO else "train"
                entry = create_dataset_entry(user_input, args, split)
                valid_samples.append(entry)
                pbar.update(1)
    
    pbar.close()

    # å¯«å…¥æª”æ¡ˆ
    print("\nğŸ’¾ å¯«å…¥æª”æ¡ˆä¸­...")
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        for entry in valid_samples:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")

    # çµ±è¨ˆçµæœ
    train_count = sum(1 for e in valid_samples if e["metadata"] == "train")
    eval_count = sum(1 for e in valid_samples if e["metadata"] == "eval")
    
    print(f"\nâœ… ç”Ÿæˆå®Œæˆï¼æª”æ¡ˆå·²å„²å­˜è‡³: {OUTPUT_FILE}")
    print(f"   ç¸½è¨ˆ: {len(valid_samples)} ç­† | è¨“ç·´é›†: {train_count} ç­† | é©—è­‰é›†: {eval_count} ç­†")

if __name__ == "__main__":
    main()